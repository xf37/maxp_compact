{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-p Regionalization with Compactness Constraints\n",
    "\n",
    "This notebook demonstrates how to use the max-p regionalization algorithm with compactness optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "First, install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you need to install dependencies\n",
    "# !pip install numpy scipy pandas geopandas libpysal pysal matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import libpysal\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from copy import deepcopy\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.insert(0, '../src')\n",
    "from compactness import Polygon, shpProc, Region, Partition\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Sample Data\n",
    "\n",
    "We'll use the Mexico dataset from libpysal as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load sample data from libpysal\n",
    "mexico = libpysal.examples.load_example('mexico')\n",
    "mexico_shp = mexico.get_path('mexicojoin.shp')\n",
    "\n",
    "# Read the shapefile\n",
    "gdf = gpd.read_file(mexico_shp)\n",
    "print(f\"Loaded {len(gdf)} polygons\")\n",
    "print(f\"Columns: {list(gdf.columns)}\")\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "gdf.plot(ax=ax, edgecolor='black', facecolor='lightblue')\n",
    "ax.set_title('Mexico States')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm parameters\n",
    "flag_rg = 1        # Enable compactness in region growth (0=off, 1=on)\n",
    "flag_ea = 1        # Enable compactness in enclave assignment (0=off, 1=on)\n",
    "flag_ls = 1        # Enable compactness in local search (0=off, 1=on)\n",
    "randomGrow = 2     # Number of top candidates for random selection during growth\n",
    "randomAssign = 2   # Number of top candidates for random selection during enclave assignment\n",
    "\n",
    "# Threshold - minimum attribute value for each region\n",
    "# Using PCGDP1940 (Per Capita GDP 1940) as threshold attribute\n",
    "threshold = 5000   # Adjust based on your data\n",
    "\n",
    "# Construction parameters\n",
    "ITERCONSTRUCT = 100   # Number of construction iterations (reduce for faster demo)\n",
    "ITERSA = 10           # Number of simulated annealing iterations\n",
    "\n",
    "# Local search parameters\n",
    "alpha = 0.998         # Cooling rate\n",
    "tabuLength = 10       # Tabu list length\n",
    "max_no_move = 50      # Maximum non-improving moves\n",
    "\n",
    "# Attribute names (from Mexico dataset)\n",
    "threshold_name = 'PCGDP1940'   # Threshold attribute\n",
    "attrs_name_str = 'PCGDP1950'   # Attribute for heterogeneity calculation\n",
    "attrs_name = [attrs_name_str]\n",
    "\n",
    "print(f\"Threshold: {threshold}\")\n",
    "print(f\"Threshold attribute: {threshold_name}\")\n",
    "print(f\"Heterogeneity attribute: {attrs_name_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for data path\n",
    "os.environ['MAXP_DATA_PATH'] = os.path.dirname(mexico_shp) + '/'\n",
    "\n",
    "# Create spatial weights matrix (Queen contiguity)\n",
    "w = libpysal.weights.Queen.from_dataframe(gdf)\n",
    "print(f\"Spatial weights created: {w.n} observations\")\n",
    "\n",
    "# Calculate distance matrix for heterogeneity\n",
    "attr = gdf[attrs_name].values\n",
    "distance_matrix = squareform(pdist(attr, metric='cityblock'))\n",
    "print(f\"Distance matrix shape: {distance_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Shape Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process shapes and calculate moment of inertia\n",
    "print(\"Processing shapes...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "Shp = shpProc(mexico_shp, attrs_name_str, threshold_name).polygons\n",
    "n_polygons = len(Shp)\n",
    "\n",
    "print(f\"Processed {n_polygons} polygons\")\n",
    "print(f\"Time elapsed: {datetime.now() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Region Growth Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction phase\n",
    "print(\"Starting construction phase...\")\n",
    "maxp_time1 = datetime.now()\n",
    "\n",
    "max_p = 0\n",
    "partitions_list = []\n",
    "arr = np.arange(0, n_polygons)\n",
    "partition_id = 0\n",
    "\n",
    "for iteration in range(ITERCONSTRUCT):\n",
    "    if iteration % 20 == 0:\n",
    "        print(f\"Iteration {iteration}/{ITERCONSTRUCT}\")\n",
    "    \n",
    "    C = 0  # Region counter\n",
    "    enclaveList = []\n",
    "    np.random.shuffle(arr)\n",
    "    APartition = Partition(n_polygons)\n",
    "    \n",
    "    for index in range(n_polygons):\n",
    "        P = arr[index]\n",
    "        \n",
    "        if APartition.label[P] != 0:\n",
    "            continue\n",
    "        \n",
    "        # Grow region from seed P\n",
    "        C += 1\n",
    "        APartition.label[P] = C\n",
    "        ARegion = Region(Shp, P, C, w, APartition.label)\n",
    "        \n",
    "        while len(ARegion.NeighborPolysID) > 0:\n",
    "            if ARegion.isRegion(threshold):\n",
    "                break\n",
    "            combined_unit = ARegion.selectUnit_growRegion(Shp, APartition.label, w, flag_rg, randomGrow)\n",
    "            APartition.label[combined_unit.id] = C\n",
    "        \n",
    "        if not ARegion.isRegion(threshold):\n",
    "            C -= 1\n",
    "            enclaveList = enclaveList + list(ARegion.data)\n",
    "        else:\n",
    "            ARegion.withinRegionHetero = ARegion.calculateWithinRegionHetero(distance_matrix)\n",
    "            APartition.p += 1\n",
    "            APartition.data.append(ARegion)\n",
    "    \n",
    "    # Update max_p and handle enclaves\n",
    "    if APartition.p >= max_p:\n",
    "        max_p = APartition.p\n",
    "        \n",
    "        # Assign enclaves\n",
    "        enclave_index = 0\n",
    "        while len(enclaveList) > 0:\n",
    "            regionList = []\n",
    "            AEnclave = enclaveList[enclave_index]\n",
    "            ecNeighbors = w.neighbors[AEnclave]\n",
    "            \n",
    "            for ecn in ecNeighbors:\n",
    "                if ecn not in enclaveList and APartition.label[ecn] not in regionList:\n",
    "                    regionList.append(APartition.label[ecn])\n",
    "            \n",
    "            if len(regionList) == 0:\n",
    "                enclave_index += 1\n",
    "            else:\n",
    "                if flag_ea == 1:\n",
    "                    # Compactness-aware assignment\n",
    "                    updatedRegionList = []\n",
    "                    for regionID in regionList:\n",
    "                        region = APartition.data[regionID - 1]\n",
    "                        DiffShapeindex, rshape, shapeindex = region.enclaveAssign(Shp[AEnclave])\n",
    "                        updatedRegionList.append((regionID, DiffShapeindex, rshape, shapeindex))\n",
    "                    \n",
    "                    updatedRegionList = sorted(updatedRegionList, key=lambda tup: tup[1])\n",
    "                    top_n = min(len(updatedRegionList), randomAssign)\n",
    "                    unit_index = np.random.randint(top_n) if top_n > 0 else 0\n",
    "                    \n",
    "                    min_region = updatedRegionList[unit_index][0]\n",
    "                    min_rshape = updatedRegionList[unit_index][2]\n",
    "                else:\n",
    "                    # Random assignment\n",
    "                    regionindex = np.random.randint(len(regionList))\n",
    "                    min_region = regionList[regionindex]\n",
    "                    region = APartition.data[min_region - 1]\n",
    "                    _, min_rshape, _ = region.enclaveAssign(Shp[AEnclave])\n",
    "                \n",
    "                APartition.label[AEnclave] = min_region\n",
    "                updateRegion = APartition.data[min_region - 1]\n",
    "                updateRegion.data.add(AEnclave)\n",
    "                updateRegion.area = min_rshape[1]\n",
    "                updateRegion.centroidX = min_rshape[2]\n",
    "                updateRegion.centroidY = min_rshape[3]\n",
    "                updateRegion.inertia = min_rshape[0]\n",
    "                updateRegion.spatialAttrTotal += Shp[AEnclave].threshold\n",
    "                updateRegion.withinRegionHetero = updateRegion.calculateWithinRegionHetero(distance_matrix)\n",
    "                del enclaveList[enclave_index]\n",
    "                enclave_index = 0\n",
    "        \n",
    "        APartition.shapeindex = APartition.calculateShapeIndex()\n",
    "        APartition.totalwithinRegionHetero = APartition.calculateWithinRegionHeteroTotal(distance_matrix)\n",
    "        APartition.id = partition_id\n",
    "        partitions_list.append(APartition)\n",
    "        partition_id += 1\n",
    "\n",
    "maxp_time2 = datetime.now()\n",
    "print(f\"\\nConstruction completed in {maxp_time2 - maxp_time1}\")\n",
    "print(f\"Maximum p found: {max_p}\")\n",
    "print(f\"Number of partitions: {len(partitions_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Find Best Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the partition with best compactness\n",
    "minPartitionShape = 1.0\n",
    "bestPartition = None\n",
    "\n",
    "for partition in partitions_list:\n",
    "    if partition.p == max_p and partition.shapeindex < minPartitionShape:\n",
    "        bestPartition = partition\n",
    "        minPartitionShape = partition.shapeindex\n",
    "\n",
    "print(f\"Best partition shape index: {minPartitionShape:.4f}\")\n",
    "print(f\"Number of regions: {bestPartition.p}\")\n",
    "print(f\"Total within-region heterogeneity: {bestPartition.totalwithinRegionHetero:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add region labels to geodataframe\n",
    "gdf['region'] = bestPartition.label\n",
    "\n",
    "# Plot the regionalization result\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original data\n",
    "gdf.plot(column=threshold_name, ax=axes[0], legend=True, \n",
    "         cmap='YlOrRd', edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_title(f'Original Data: {threshold_name}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Regionalization result\n",
    "gdf.plot(column='region', ax=axes[1], categorical=True, \n",
    "         cmap='Set3', edgecolor='black', linewidth=0.5, legend=True)\n",
    "axes[1].set_title(f'Max-p Regionalization (p={max_p}, Shape Index={minPartitionShape:.4f})')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary for each region\n",
    "print(\"Region Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Region':<10} {'Units':<10} {'Threshold Sum':<15} {'Shape Index':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, region in enumerate(bestPartition.data):\n",
    "    print(f\"{i+1:<10} {len(region.data):<10} {region.spatialAttrTotal:<15.2f} {region.shapeindex:<15.4f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total':<10} {n_polygons:<10} {'':<15} {minPartitionShape:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to files\n",
    "output_dir = '../results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save as shapefile\n",
    "gdf.to_file(f'{output_dir}/maxp_result.shp')\n",
    "print(f\"Saved shapefile to {output_dir}/maxp_result.shp\")\n",
    "\n",
    "# Save summary as CSV\n",
    "summary = pd.DataFrame({\n",
    "    'max_p': [max_p],\n",
    "    'shape_index': [minPartitionShape],\n",
    "    'total_heterogeneity': [bestPartition.totalwithinRegionHetero],\n",
    "    'threshold': [threshold]\n",
    "})\n",
    "summary.to_csv(f'{output_dir}/summary.csv', index=False)\n",
    "print(f\"Saved summary to {output_dir}/summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "The max-p regionalization with compactness constraints has been completed. You can adjust the parameters in Section 4 to experiment with different settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
